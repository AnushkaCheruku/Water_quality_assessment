# -*- coding: utf-8 -*-
"""usability

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IxZgCuhBim0lWpxl-aDNg8gTNl7dRpKO
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv(r'/content/water_potability.csv')
df.head()

df.shape

df.isnull().sum()

df.info()

df.describe()

df['Sulfate'].mean()

df.fillna(df.mean(), inplace=True)
df.head()

print(df)

df.isnull().sum()

df.info()

df.describe()

df.Potability.value_counts()

df['Solids'].describe()

X = df.drop('Potability',axis=1)
Y= df['Potability']

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size= 0.2, random_state=101,shuffle=True)

Y_train.value_counts()

Y_test.value_counts()

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
dt=DecisionTreeClassifier(criterion= 'gini', min_samples_split= 10, splitter= 'best')
dt.fit(X_train,Y_train)

prediction=dt.predict(X_test)
accuracy_dt=accuracy_score(Y_test,prediction)*100
accuracy_dt

print("Accuracy on training set: {:.3f}".format(dt.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(dt.score(X_test, Y_test)))

accuracy_score(prediction,Y_test)

print("Feature importances:\n{}".format(dt.feature_importances_))

confusion_matrix(prediction,Y_test)

X_DT=dt.predict([[5.735724, 158.318741,25363.016594,7.728601,377.543291,568.304671,13.626624,75.952337,4.732954]])

X_DT

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(metric='manhattan', n_neighbors=22)
knn.fit(X_train,Y_train)

prediction_knn=knn.predict(X_test)
accuracy_knn=accuracy_score(Y_test,prediction_knn)*100
print('accuracy_score score     : ',accuracy_score(Y_test,prediction_knn)*100,'%')

confusion_matrix(prediction,Y_test)

dt.get_params().keys()

#example of grid searching key hyperparametres for logistic regression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = DecisionTreeClassifier()
criterion = ["gini", "entropy"]
splitter = ["best", "random"]
min_samples_split = [2,4,6,8,10]

# define grid search
grid = dict(splitter=splitter, criterion=criterion, min_samples_split=min_samples_split)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search_dt = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='accuracy',error_score=0)
grid_search_dt.fit(X_train, Y_train)

# summarize results
print(f"Best: {grid_search_dt.best_score_:.3f} using {grid_search_dt.best_params_}")
means = grid_search_dt.cv_results_['mean_test_score']
stds = grid_search_dt.cv_results_['std_test_score']
params = grid_search_dt.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

print("Training Score:",grid_search_dt.score(X_train, Y_train)*100)
print("Testing Score:", grid_search_dt.score(X_test, Y_test)*100)

import sklearn
from sklearn.metrics import  make_scorer
from sklearn.model_selection import cross_val_score

def classification_report_with_accuracy_score(Y_test, y_pred2):
    print (sklearn.metrics.classification_report(Y_test, y_pred2)) # print classification report
    return accuracy_score(Y_test, y_pred2) # return accuracy score


nested_score = cross_val_score(grid_search_dt, X=X_train, y=Y_train, cv=cv,
               scoring=make_scorer(classification_report_with_accuracy_score))
print (nested_score)

dt_y_predicted = grid_search_dt.predict(X_test)
dt_y_predicted

grid_search_dt.best_params_

dt_grid_score=accuracy_score(Y_test, dt_y_predicted)
dt_grid_score

confusion_matrix(Y_test, dt_y_predicted)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = KNeighborsClassifier()
n_neighbors = range(1, 31)
weights = ['uniform', 'distance']
metric = ['euclidean', 'manhattan', 'minkowski']

# define grid search
grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)
grid_search_knn = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='accuracy',error_score=0)
grid_search_knn.fit(X_train, Y_train)

# summarize results
print(f"Best: {grid_search_knn.best_score_:.3f} using {grid_search_knn.best_params_}")
means = grid_search_knn.cv_results_['mean_test_score']
stds = grid_search_knn.cv_results_['std_test_score']
params = grid_search_knn.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

knn_y_predicted = grid_search_knn.predict(X_test)

knn_y_predicted

knn_grid_score=accuracy_score(Y_test, knn_y_predicted)

knn_grid_score

grid_search_knn.best_params_

confusion_matrix(Y_test, knn_y_predicted)

X_KNN=knn.predict([[5.735724, 158.318741,25363.016594,7.728601,377.543291,568.304671,13.626624,75.952337,4.732954]])

X_KNN

pip install pycaret

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from pycaret.classification import *

# Load the dataset
df = pd.read_csv('/content/water_potability.csv')

# Fill missing values with mean
df.fillna(df.mean(), inplace=True)

# Split data into X (features) and Y (target)
X = df.drop('Potability', axis=1)
Y = df['Potability']

# Split data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101, shuffle=True)

# Initialize PyCaret setup
exp = setup(data=pd.concat([X_train, Y_train], axis=1), target='Potability', session_id=123)

# Compare models and select the best one
best_model = compare_models()

# Optionally, tune the best model
tuned_best_model = tune_model(best_model)

# Predict on test set using the best model
predictions = predict_model(tuned_best_model, data=X_test)

# Evaluate predictions
accuracy_pycaret = accuracy_score(predictions['Potability'], Y_test) * 100
print(f"Accuracy using PyCaret: {accuracy_pycaret:.2f}%")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from pycaret.classification import *

# Load the dataset
df = pd.read_csv('/content/water_potability.csv')

# Check columns and first few rows to verify data
print("Columns:", df.columns)
print("First few rows:\n", df.head())

# Fill missing values with mean
df.fillna(df.mean(), inplace=True)

# Split data into X (features) and Y (target)
X = df.drop('Potability', axis=1)
Y = df['Potability']

# Split data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101, shuffle=True)

# Initialize PyCaret setup
exp = setup(data=pd.concat([X_train, Y_train], axis=1), target='Potability', session_id=123)

# Compare models and select the best one
best_model = compare_models()

# Optionally, tune the best model
tuned_best_model = tune_model(best_model)

# Predict on test set using the best model
predictions = predict_model(tuned_best_model, data=X_test)

# Inspect the structure of predictions DataFrame
print(predictions.head())

# Evaluate predictions
accuracy_pycaret = accuracy_score(predictions['Potability'], Y_test) * 100
print(f"Accuracy using PyCaret: {accuracy_pycaret:.2f}%")

